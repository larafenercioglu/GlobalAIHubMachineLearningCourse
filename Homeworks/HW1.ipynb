{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)How would you define Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is an application of AI that provides the systems to learn and improve themselves by experience without explicitly being programmed. These systems learn automatically without any human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning, we gave the algorithm labeled data. In other words, we have a prior knowledge of what the output values for our samples should be and therefore our aim is to create a model that learns from those values. Regression is used when our label is numeric and classification is used when our label is categorical. These are the example algorithms for supervised learning. On the other hand, we provide unlabeled data in unsupervised learning. We don't have any labels and the main goal is to discover the hidden patterns in the data. Clustering is one example for this learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)What are the test and validation set, and why would you want to use them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set is helpful while tuning the model's hyperparameters. Test set is used to test how well our model predicts. We use these sets to teach and improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to determine the duplicate values and duplicates should be removed. Then, we search for imbalanced data by oversampling or undersampling the selected data. Afterwards, we determine the missing values. These missing values can be eliminated or can be replaced by the mean or median. If the distribution of the data in graph is normal then, we can replace the missing values with the mean however, if the graph is negatively or positively skewed then, we replace with the median. Another thing that we should do in the preprocessing step is to determine outliers because these outliers can mislead the model while predicting. By looking at the standart deviations we can eliminate whose absolute value of the standart deviation is greater than 3. IQR Calculation is also eliminates the outliers. Also we can apply Feature Scaling to the dataset by either using normalization or standardization. Bucketing (Binning) is another step that we can apply in order to minimize the effects of small observation errors. We can divide the data into small intervals and then replace these intervals with a general value calculated for that interval. Feature Extraction can be applied for transforming raw data into numerical features that can be processed while preserving the information in the original data set. We can create new features that will benefit our model. Moreover, Feature Encoding is used for performing transformations on the data such that it can be easily accepted as input for machine learning algorithms while still retaining its original meaning. This is used while handling with categorical variables. Nominal values can be used after one-to-one mapping is processed on that data. Ordinal values can be used after encoded to numerical values. Train, Validation, Test Split are important as well while preprocessing the data because we need to train our model with the data, optionally we can use validation data while tuning the model's hyperparameters and we need to test our model to see if we have done a good job. Cross Validation is another way to preprocess the data that helps us to overcome overfitting issue. We need to consider these preprocessing steps because data preparation ensures accuracy in the data, which leads to accurate insights. Without data preparation, itâ€™s possible that insights will be off due to noise data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5) How you can explore countionus and discrete variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A discrete variable is a variable whose value is obtained by counting whereas continuous variable relates to change over time, involving concepts that are not simply countable but require detailed measurements. By plotting these variables we can see that continuous variables follow a range of results, and discrete variables show individual points on the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous variable is present in the plot. Bar charts are used. Maybe by appling binning we can preprocess the data. Since there are couple of noisy datas, we can minimize the effect of these small observation errors by bucketing. The original data values cab be divided into small intervals known as bins and then they can be replaced by a general value calculated for that bin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
